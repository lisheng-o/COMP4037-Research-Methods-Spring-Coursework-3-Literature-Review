const generatedBibEntries = {
    "10.1145/3571730": {
        "abstract": "Natural Language Generation (NLG) has improved exponentially in recent years thanks to the development of sequence-to-sequence deep learning technologies such as Transformer-based language models. This advancement has led to more fluent and coherent NLG, leading to improved development in downstream tasks such as abstractive summarization, dialogue generation, and data-to-text generation. However, it is also apparent that deep learning based generation is prone to hallucinate unintended text, which degrades the system performance and fails to meet user expectations in many real-world scenarios. To address this issue, many studies have been presented in measuring and mitigating hallucinated texts, but these have never been reviewed in a comprehensive manner before.In this survey, we thus provide a broad overview of the research progress and challenges in the hallucination problem in NLG. The survey is organized into two parts: (1) a general overview of metrics, mitigation methods, and future directions, and (2) an overview of task-specific research progress on hallucinations in the following downstream tasks, namely abstractive summarization, dialogue generation, generative question answering, data-to-text generation, and machine translation. This survey serves to facilitate collaborative efforts among researchers in tackling the challenge of hallucinated texts in NLG.",
        "address": "New York, NY, USA",
        "articleno": "248",
        "author": "Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Ye Jin and Madotto, Andrea and Fung, Pascale",
        "doi": "10.1145/3571730",
        "issn": "0360-0300",
        "issue_date": "December 2023",
        "journal": "ACM Comput. Surv.",
        "keywords": "consistency in NLG, factuality in NLG, faithfulness in NLG, extrinsic hallucination, intrinsic hallucination, Hallucination",
        "month": "mar,",
        "number": "12",
        "numpages": "38",
        "publisher": "Association for Computing Machinery",
        "title": "Survey of Hallucination in Natural Language Generation",
        "type": "article",
        "url": "https://doi.org/10.1145/3571730",
        "volume": "55",
        "year": "2023"
    },
    "10.5555/3495724.3496517": {
        "abstract": "Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) \u2014 models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, and another which can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state of the art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.",
        "address": "Red Hook, NY, USA",
        "articleno": "793",
        "author": "Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K\\\"{u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt\\\"{a}schel, Tim and Riedel, Sebastian and Kiela, Douwe",
        "booktitle": "Proceedings of the 34th International Conference on Neural Information Processing Systems",
        "doi": "https://doi.org/10.48550/arXiv.2005.11401",
        "isbn": "9781713829546",
        "keywords": "",
        "location": "Vancouver, BC, Canada",
        "numpages": "16",
        "publisher": "Curran Associates Inc.",
        "series": "NIPS '20",
        "title": "Retrieval-augmented generation for knowledge-intensive NLP tasks",
        "type": "inproceedings",
        "year": "2020"
    },
    "10447898": {
        "abstract": "Despite recent progress in text-to-audio (TTA) generation, we show that the state-of-the-art models, such as AudioLDM, trained on datasets with an imbalanced class distribution, such as AudioCaps, are biased in their generation performance. Specifically, they excel in generating common audio classes while underperforming in the rare ones, thus degrading the overall generation performance. We refer to this problem as long-tailed text-to-audio generation. To address this issue, we propose a simple retrieval-augmented approach for TTA models. Specifically, given an input text prompt, we first leverage a Contrastive Language Audio Pretraining (CLAP) model to retrieve relevant text-audio pairs. The features of the retrieved audio-text data are then used as additional conditions to guide the learning of TTA models. We enhance AudioLDM with our proposed approach and denote the resulting augmented system as Re-AudioLDM. On the AudioCaps dataset, Re-AudioLDM achieves a state-of-the-art Frechet Audio Distance (FAD) of 1.37, outperforming the existing approaches by a large margin. Furthermore, we show that Re-AudioLDM can generate realistic audio for complex scenes, rare audio classes, and even unseen audio types, indicating its potential in TTA tasks.",
        "author": "Yuan, Yi and Liu, Haohe and Liu, Xubo and Huang, Qiushi and Plumbley, Mark D. and Wang, Wenwu",
        "booktitle": "ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
        "doi": "10.1109/ICASSP48485.2024.10447898",
        "issn": "2379-190X",
        "keywords": "Measurement;Tail;Signal processing;Data models;Acoustics;Task analysis;Speech processing;Audio generation;retrieval-information;diffusion model;deep learning;long tail problem",
        "month": "April",
        "number": "",
        "pages": "581-585",
        "title": "Retrieval-Augmented Text-to-Audio Generation",
        "type": "INPROCEEDINGS",
        "volume": "",
        "year": "2024"
    },
    "Gao2022RetrievalAugmentedMK": {
        "author": "Yifan Gao and Qingyu Yin and Zheng Li and Rui Meng and Tong Zhao and Bing Yin and Irwin King and Michael R. Lyu",
        "journal": "ArXiv",
        "title": "Retrieval-Augmented Multilingual Keyphrase Generation with Retriever-Generator Iterative Training",
        "type": "article",
        "url": "https://api.semanticscholar.org/CorpusID:248986542",
        "volume": "abs/2205.10471",
        "year": "2022"
    },
    "huang-etal-2022-concrete": {
        "abstract": "Fact-checking has gained increasing attention due to the widespread of falsified information. Most fact-checking approaches focus on claims made in English only due to the data scarcity issue in other languages. The lack of fact-checking datasets in low-resource languages calls for an effective cross-lingual transfer technique for fact-checking. Additionally, trustworthy information in different languages can be complementary and helpful in verifying facts. To this end, we present the first fact-checking framework augmented with cross-lingual retrieval that aggregates evidence retrieved from multiple languages through a cross-lingual retriever. Given the absence of cross-lingual information retrieval datasets with claim-like queries, we train the retriever with our proposed Cross-lingual Inverse Cloze Task (X-ICT), a self-supervised algorithm that creates training instances by translating the title of a passage. The goal for X-ICT is to learn cross-lingual retrieval in which the model learns to identify the passage corresponding to a given translated title. On the X-Fact dataset, our approach achieves 2.23{\\%} absolute F1 improvement in the zero-shot cross-lingual setup over prior systems. The source code and data are publicly available at \\url{https://github.com/khuangaf/CONCRETE}.",
        "address": "Gyeongju, Republic of Korea",
        "author": "Huang, Kung-Hsiang  and Zhai, ChengXiang  and Ji, Heng",
        "booktitle": "Proceedings of the 29th International Conference on Computational Linguistics",
        "editor": "Calzolari, Nicoletta  and Huang, Chu-Ren  and Kim, Hansaem  and Pustejovsky, James  and Wanner, Leo  and Choi, Key-Sun  and Ryu, Pum-Mo  and Chen, Hsin-Hsi  and Donatelli, Lucia  and Ji, Heng  and Kurohashi, Sadao  and Paggio, Patrizia  and Xue, Nianwen  and Kim, Seokhwan  and Hahm, Younggyun  and He, Zhong  and Lee, Tony Kyungil  and Santus, Enrico  and Bond, Francis  and Na, Seung-Hoon",
        "month": "oct",
        "pages": "1024--1035",
        "publisher": "International Committee on Computational Linguistics",
        "title": "CONCRETE: Improving Cross-lingual Fact-checking with Cross-lingual Retrieval",
        "type": "inproceedings",
        "url": "https://aclanthology.org/2022.coling-1.86/",
        "year": "2022"
    },
    "izacard2021leveragingpassageretrievalgenerative": {
        "archiveprefix": "arXiv",
        "author": "Gautier Izacard and Edouard Grave",
        "eprint": "2007.01282",
        "primaryclass": "cs.CL",
        "title": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering",
        "type": "misc",
        "url": "https://arxiv.org/abs/2007.01282",
        "year": "2021"
    },
    "karpukhin2020densepassageretrievalopendomain": {
        "archiveprefix": "arXiv",
        "author": "Vladimir Karpukhin and Barlas O\u011fuz and Sewon Min and Patrick Lewis and Ledell Wu and Sergey Edunov and Danqi Chen and Wen-tau Yih",
        "eprint": "2004.04906",
        "primaryclass": "cs.CL",
        "title": "Dense Passage Retrieval for Open-Domain Question Answering",
        "type": "misc",
        "url": "https://arxiv.org/abs/2004.04906",
        "year": "2020"
    },
    "liu2021kgbartknowledgegraphaugmentedbart": {
        "archiveprefix": "arXiv",
        "author": "Ye Liu and Yao Wan and Lifang He and Hao Peng and Philip S. Yu",
        "eprint": "2009.12677",
        "primaryclass": "cs.CL",
        "title": "KG-BART: Knowledge Graph-Augmented BART for Generative Commonsense Reasoning",
        "type": "misc",
        "url": "https://arxiv.org/abs/2009.12677",
        "year": "2021"
    },
    "sun2024thinkongraphdeepresponsiblereasoning": {
        "archiveprefix": "arXiv",
        "author": "Jiashuo Sun and Chengjin Xu and Lumingyuan Tang and Saizhuo Wang and Chen Lin and Yeyun Gong and Lionel M. Ni and Heung-Yeung Shum and Jian Guo",
        "eprint": "2307.07697",
        "primaryclass": "cs.CL",
        "title": "Think-on-Graph: Deep and Responsible Reasoning of Large Language Model on Knowledge Graph",
        "type": "misc",
        "url": "https://arxiv.org/abs/2307.07697",
        "year": "2024"
    },
    "zhang-etal-2020-grounded": {
        "abstract": "Human conversations naturally evolve around related concepts and hop to distant concepts. This paper presents a new conversation generation model, ConceptFlow, which leverages commonsense knowledge graphs to explicitly model conversation flows. By grounding conversations to the concept space, ConceptFlow represents the potential conversation flow as traverses in the concept space along commonsense relations. The traverse is guided by graph attentions in the concept graph, moving towards more meaningful directions in the concept space, in order to generate more semantic and informative responses. Experiments on Reddit conversations demonstrate ConceptFlow`s effectiveness over previous knowledge-aware conversation models and GPT-2 based models while using 70{\\%} fewer parameters, confirming the advantage of explicit modeling conversation structures. All source codes of this work are available at \\url{https://github.com/thunlp/ConceptFlow}.",
        "address": "Online",
        "author": "Zhang, Houyu  and Liu, Zhenghao  and Xiong, Chenyan  and Liu, Zhiyuan",
        "booktitle": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
        "doi": "10.18653/v1/2020.acl-main.184",
        "editor": "Jurafsky, Dan  and Chai, Joyce  and Schluter, Natalie  and Tetreault, Joel",
        "month": "jul",
        "pages": "2031--2043",
        "publisher": "Association for Computational Linguistics",
        "title": "Grounded Conversation Generation as Guided Traverses in Commonsense Knowledge Graphs",
        "type": "inproceedings",
        "url": "https://aclanthology.org/2020.acl-main.184/",
        "year": "2020"
    }
};